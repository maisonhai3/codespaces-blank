# Logstash configuration optimized for 10K logs/sec
input {
  file {
    path => "/shared-logs/aiscout.log"
    start_position => "beginning"
    sincedb_path => "/dev/null"
    codec => "json"
    tags => ["aiscout"]
    # High-throughput optimizations
    file_chunk_size => 131072
    file_chunk_count => 256
    delimiter => "\n"
    check_archive_validity => false
    discover_interval => 0.1
    stat_interval => 0.1
    close_older => 30
    max_open_files => 4096
  }
}

filter {
  if [tags] and "aiscout" in [tags] {
    # Minimal processing for maximum speed
    mutate {
      add_field => { "logtype" => "aiscout" }
      remove_tag => [ "_grokparsefailure" ]
    }
    
    # Only add timestamp if absolutely needed
    if ![timestamp] {
      ruby {
        code => "event.set('timestamp', Time.now.utc.iso8601)"
      }
    }
  }
}

output {
  elasticsearch {
    hosts => ["elasticsearch:9200"]
    index => "logstash-logs-%{+YYYY.MM.dd}"
    # High-throughput bulk optimization
    bulk_max_size => 5000
    flush_size => 5000
    idle_flush_time => 0.1
    workers => 8
    # Connection optimization for high throughput
    pool_max => 2000
    pool_max_per_route => 200
    resurrect_delay => 0.5
    retry_max_interval => 3
    sniffing => false
    # Disable compression for maximum speed
    compression => "none"
    # Authentication
    user => "elastic"
    password => "changeme"
    # SSL/TLS
    ssl => false
    ssl_certificate_verification => false
    # HTTP settings
    timeout => 60
    # Connection pooling
    keepalive => true
    keepalive_timeout => 30
  }
}
